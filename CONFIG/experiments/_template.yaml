# ============================================================================
# EXPERIMENT CONFIG TEMPLATE
# ============================================================================
# This is a SELF-CONTAINED config file - everything you need is here!
# Copy this file and rename it to create a new experiment config.
# Example: cp _template.yaml my_experiment.yaml
#
# IMPORTANT: Experiment configs OVERRIDE all other configs (highest priority)
# You don't need to edit any other files - just this one!
# ============================================================================

experiment:
  name: my_experiment  # Change this to your experiment name
  description: "Description of your experiment"

# ============================================================================
# DATA CONFIGURATION
# ============================================================================
# All data-related settings - overrides pipeline/training/intelligent.yaml
data:
  # Data directory (relative to repo root or absolute path)
  data_dir: data/data_labeled_v2/interval=5m
  
  # ========================================================================
  # SYMBOLS CONFIGURATION
  # ========================================================================
  # Option 1: Explicit symbols list
  symbols: [AAPL, MSFT, GOOGL, TSLA, NVDA]
  
  # Option 2: Auto-discover symbols from data_dir
  # symbols: []  # Empty = auto-discover all symbols from data_dir
  # symbol_batch_size: 15  # Optional: limit to N symbols (random sample, reproducible)
  #
  # Auto-discovery semantics:
  # - symbols: [] or symbols: null or missing → auto-discover from data_dir
  # - symbols: [AAPL, MSFT, ...] → use explicit list (no discovery)
  #
  # Path semantics:
  # - If data_dir contains "interval=5m", discovery uses it directly
  # - If data_dir is root, provide interval: 5m and discovery appends interval=5m
  # - If data_dir has interval=X but you specify interval=Y, raises error (mismatch)
  
  # Bar interval (5m, 15m, 1h, etc.)
  # Not needed if data_dir already contains interval=5m
  interval: 5m  # or bar_interval: 5m (both work)
  
  # Maximum samples per symbol (caps data to prevent memory issues)
  max_samples_per_symbol: 50000
  
  # Maximum rows per symbol (additional cap for data loading)
  max_rows_per_symbol: 50000
  
  # Maximum rows for training (caps training data)
  max_rows_train: 100000
  
  # Cross-sectional sampling limits
  max_cs_samples: 10000  # Max cross-sectional samples per timestamp
  min_cs: 10             # Minimum cross-sectional samples per timestamp

# ============================================================================
# INTELLIGENT TRAINING CONFIGURATION
# ============================================================================
# All target/feature selection and training settings
# This section OVERRIDES pipeline/training/intelligent.yaml
intelligent_training:
  # ========================================================================
  # TARGET SELECTION
  # ========================================================================
  # Auto-discover targets from data (true) or use manual_targets list (false)
  auto_targets: true
  
  # If auto_targets=true: number of top targets to SELECT after ranking
  top_n_targets: 10
  
  # Maximum targets to EVALUATE during ranking (limits evaluation for faster testing)
  # Set lower for faster runs, higher for comprehensive evaluation
  max_targets_to_evaluate: 50
  
  # Manual target list (used when auto_targets=false)
  # Examples:
  #   - fwd_ret_60m (forward return - non-repainting, honest baseline)
  #   - fwd_ret_30m, fwd_ret_120m (multiple forward return horizons)
  #   - y_first_touch_60m_0.8 (triple barrier - which barrier hits first)
  #   - y_will_peak_60m_0.8 (peak detection - may repaint)
  #   - y_will_valley_60m_0.8 (valley detection - may repaint)
  # manual_targets: [fwd_ret_60m]
  
  # Exclude target patterns (targets matching any pattern will be excluded)
  # Patterns are matched as substrings (e.g., "will_peak" matches "y_will_peak_60m_0.8")
  exclude_target_patterns:
    - "will_peak"   # Exclude peak detection targets (may repaint)
    - "will_valley" # Exclude valley detection targets (may repaint)
    # - "fwd_ret_20d"  # Uncomment to exclude 20-day targets
    # - "fwd_ret_15d"  # Uncomment to exclude 15-day targets
  
  # ========================================================================
  # FEATURE SELECTION
  # ========================================================================
  # Auto-discover features (true) or use manual_features list (false)
  auto_features: true
  
  # Number of top features to select per target
  top_m_features: 50
  
  # Manual feature list (used when auto_features=false)
  # manual_features: [feature1, feature2, ...]
  
  # ========================================================================
  # TRAINING SETTINGS
  # ========================================================================
  # Training strategy: single_task, multi_task, cascade
  strategy: single_task
  
  # Run leakage diagnostics (slower but more thorough)
  run_leakage_diagnostics: false

# ============================================================================
# MODEL FAMILIES CONFIGURATION
# ============================================================================
# Which models to use for feature selection and training
# Available: lightgbm, xgboost, random_forest, neural_network, catboost,
#            lasso, mutual_information, univariate_selection, rfe, boruta,
#            stability_selection

feature_selection:
  # Model families to use for feature selection
  model_families:
    - lightgbm
    - xgboost
    - random_forest
    - catboost
    - neural_network
    - lasso
    - mutual_information
    - univariate_selection

training:
  # Model families to train (usually same as feature selection)
  model_families:
    - lightgbm
    - xgboost
    - random_forest
    - catboost
    - neural_network
    - lasso
    - mutual_information
    - univariate_selection

# ============================================================================
# PARALLEL EXECUTION CONFIGURATION
# ============================================================================
# Control parallel execution for faster runs
# These settings OVERRIDE pipeline/threading.yaml

# Multi-target parallel execution (for faster target evaluation)
multi_target:
  # Enable parallel target evaluation (ProcessPoolExecutor)
  parallel_targets: true
  
  # Continue to next target if one fails
  skip_on_error: true
  
  # Save multi_target_summary.json
  save_summary: true

# Feature selection parallel execution
multi_model_feature_selection:
  # Enable parallel symbol processing (ProcessPoolExecutor)
  parallel_symbols: true

# Threading/parallel worker configuration
# These override pipeline/threading.yaml settings
threading:
  parallel:
    # Max workers for ProcessPoolExecutor (CPU-bound tasks like target evaluation)
    # Set to null for auto-detect, or specific number (e.g., 4, 8)
    max_workers_process: null  # null = auto-detect based on CPU cores
    
    # Max workers for ThreadPoolExecutor (I/O-bound tasks)
    max_workers_thread: null    # null = auto-detect
    
    # Master switch for parallel execution
    enabled: true

# ============================================================================
# DECISION CONFIGURATION (Auto-Config Application)
# ============================================================================
# Control automatic config updates based on run results
# These settings OVERRIDE pipeline/training/decisions.yaml

decisions:
  # Application mode: "off" (assist mode only), "dry_run" (show patch), "apply" (auto-apply)
  apply_mode: "off"  # Start with "off", then "dry_run", then "apply" when confident
  
  # Minimum decision level to apply (0=no action, 1=warning, 2=recommendation, 3=action)
  min_level_to_apply: 2
  
  # Bayesian patch policy (Thompson sampling over discrete patch templates)
  use_bayesian: false  # Set to true to enable automatic config optimization
  
  # Bayesian settings (only used if use_bayesian: true)
  bayesian:
    # Minimum runs before recommending patches
    min_runs_for_learning: 5
    # Minimum P(improve) to auto-apply
    p_improve_threshold: 0.8
    # Minimum expected gain to recommend
    min_expected_gain: 0.01
    # Reward metric to optimize
    reward_metric: "cs_auc"
    # Recency decay factor (higher = more weight on recent runs)
    recency_decay: 0.95

# ============================================================================
# NOTES
# ============================================================================
# - Forward return targets (fwd_ret_*) are non-repainting and give honest baseline (~0.52-0.54)
# - Peak/valley targets (y_will_peak_*, y_will_valley_*) may repaint (scores > 0.70 suspicious)
# - First touch targets (y_first_touch_*) are generally non-repainting
# - Compare scores: honest baseline (~0.52-0.54) vs repainting targets (~0.70+) to see effect
# - Set auto_targets: false and specify manual_targets to test specific targets only
# - Use dry_run mode for decisions first, then apply when confident
# - All settings here OVERRIDE other config files - you don't need to edit anything else!
