# Transformer Model Configuration
# Time-series transformer with self-attention mechanism

model_family: "Transformer"
description: "Transformer model for sequential data"

hyperparameters:
  # Training
  epochs: 50
  batch_size: 512
  # patience: auto-injected from defaults (10)
  learning_rate: 0.001
  
  # Architecture
  d_model: 128  # Model dimension
  heads: 8  # Number of attention heads
  ff_dim: 256  # Feed-forward dimension
  dropout: 0.1
  
  # Sequence
  sequence_length: null  # Auto-detected from input

# Variants
variants:
  small:
    d_model: 64
    heads: 4
    ff_dim: 128
    dropout: 0.1
    
  medium:
    d_model: 128
    heads: 8
    ff_dim: 256
    dropout: 0.1
    
  large:
    d_model: 256
    heads: 16
    ff_dim: 512
    dropout: 0.2

