# LSTM Model Configuration
# Long Short-Term Memory for sequential data

model_family: "LSTM"
description: "LSTM model for time-series prediction"

hyperparameters:
  # Training
  epochs: 30  # Reduced from 50 to prevent timeouts
  batch_size: 256  # Reduced from 512 to speed up training
  patience: 5  # Reduced from 10 for faster early stopping
  learning_rate: 0.001
  # Dynamic batch size reduction for long sequences (>200 features)
  max_sequence_length_for_full_batch: 200
  
  # Architecture
  lstm_units: 128
  dropout: 0.2
  recurrent_dropout: 0.1
  
  # Sequence
  sequence_length: null  # Auto-detected from input

# Variants
variants:
  small:
    lstm_units: 64
    dropout: 0.1
    recurrent_dropout: 0.05
    
  medium:
    lstm_units: 128
    dropout: 0.2
    recurrent_dropout: 0.1
    
  large:
    lstm_units: 256
    dropout: 0.3
    recurrent_dropout: 0.2

