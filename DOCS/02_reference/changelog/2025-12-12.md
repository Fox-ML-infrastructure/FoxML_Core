# Changelog â€” 2025-12-12

**Sample Size Binning System, Trend Analysis System Extension (Feature Selection), Cohort-Aware Reproducibility System, RESULTS Directory Organization, Integrated Backups, Enhanced Metadata**

For a quick overview, see the [root changelog](../../../../CHANGELOG.md).  
For other dates, see the [changelog index](README.md).

---

## Added

### Sample Size Binning System

**Directory Organization by Sample Size Bins**
- **Enhancement**: Implemented audit-grade sample size binning for RESULTS directory organization
- **Structure**: `RESULTS/sample_25k-50k/{run_name}/` instead of exact `N_effective`
- **Bins**: 9 bins (0-5k, 5k-10k, 10k-25k, 25k-50k, 50k-100k, 100k-250k, 250k-500k, 500k-1M, 1M+)
- **Boundaries**: EXCLUSIVE upper bounds (`bin_min <= N_effective < bin_max`) - unambiguous binning
- **Versioning**: `sample_bin_v1` stored in metadata for backward compatibility
- **Early Estimation**: Automatically estimates `N_effective` from data files or existing metadata during initialization
- **Metadata**: Bin info stored in `metadata.json`: `bin_name`, `bin_min`, `bin_max`, `binning_scheme_version`
- **Files**:
  - `TRAINING/orchestration/intelligent_trainer.py` - Added `_get_sample_size_bin()` and `_estimate_n_effective_early()`
  - `TRAINING/utils/reproducibility_tracker.py` - Added `_compute_sample_size_bin()` static method

**Benefits**:
- Easy comparison of runs with similar sample sizes (e.g., all ~25k runs in `sample_25k-50k/`)
- Trend analysis friendly: Series won't fragment when `N_effective` jitters slightly
- Cleaner structure: Only 9 top-level directories instead of hundreds
- Audit-grade: Boundaries are deterministic, versioned, and stored in metadata
- Trend series stability: Bin NOT included in series keys (uses stable identity: cohort_id, stage, target)

### Trend Analysis System Extension

**Feature Selection Integration**
- **Enhancement**: Extended trend analysis system to feature selection (single symbol + aggregated) and cross-sectional feature ranking
- **Coverage**: Trend analysis now integrated into:
  - Target Ranking (`TRAINING/ranking/predictability/model_evaluation.py`)
  - Feature Selection - Single symbol + aggregated (`TRAINING/ranking/feature_selector.py`)
  - Cross-Sectional Feature Ranking (`TRAINING/ranking/cross_sectional_feature_ranker.py`)
- **API**: All stages use the same `log_run()` API with `RunContext` and include trend analysis automatically
- **Metrics**: Tracks `n_selected`, `n_features_selected`, `mean_consensus`, `std_consensus` for feature selection
- **CS Metrics**: Tracks `cs_importance_score`, `n_features_evaluated` for cross-sectional ranking
- **Metadata**: Trend metadata stored in `metadata.json` (same format as target ranking)
- **Fallback**: Gracefully falls back to legacy `log_comparison()` API if `RunContext` unavailable
- **Files**: 
  - `TRAINING/ranking/feature_selector.py` - Updated to use `log_run()` API with trend analysis
  - `TRAINING/ranking/cross_sectional_feature_ranker.py` - Added reproducibility tracking with trend analysis

### Files Modified

#### Feature Selection
- `TRAINING/ranking/feature_selector.py`
  - Updated reproducibility tracking to use new `log_run()` API with `RunContext`
  - Integrated trend analysis (same infrastructure as target ranking)
  - Falls back to legacy `log_comparison()` API if `RunContext` unavailable
  - Tracks metrics: `n_selected`, `n_features_selected`, `mean_consensus`, `std_consensus`

#### Cross-Sectional Feature Ranking
- `TRAINING/ranking/cross_sectional_feature_ranker.py`
  - Added `output_dir` parameter to `compute_cross_sectional_importance()`
  - Integrated reproducibility tracking with `log_run()` API and trend analysis
  - Tracks CS-specific metrics: `cs_importance_score`, `n_features_evaluated`
  - Passes `output_dir` from `feature_selector.py` for tracking

### Features

- **Trend Analysis**: Both single-symbol and cross-sectional feature selection now compute trends automatically
- **Metadata Storage**: Trend metadata stored in `metadata.json` (same format as target ranking)
- **Skip Logging**: Explicit skip reasons when insufficient runs (no silent failures)
- **Audit Reports**: Audit violations/warnings logged for feature selection runs
- **Graceful Fallback**: Falls back to legacy API if `RunContext` unavailable

### Coverage

Trend analysis is now integrated into:
- âœ… Target Ranking (`TRAINING/ranking/predictability/model_evaluation.py`)
- âœ… Feature Selection - Single symbol + aggregated (`TRAINING/ranking/feature_selector.py`)
- âœ… Cross-Sectional Feature Ranking (`TRAINING/ranking/cross_sectional_feature_ranker.py`)

All stages use the same `log_run()` API with `RunContext` and include trend analysis automatically.

### Documentation

- Updated `CHANGELOG.md` with Trend Analysis System highlight
- Updated `DOCS/03_technical/implementation/TREND_ANALYZER_VERIFICATION.md` to include feature selection coverage
- Updated `DOCS/INDEX.md` with trend analyzer verification guide reference

### Testing

To verify:
1. Run feature selection multiple times (same target, same cohort)
2. Check logs for trend messages: `ðŸ“ˆ Trend (n_selected): slope=.../day`
3. Check `metadata.json` for `trend` sections
4. Verify `TREND_REPORT.json` includes feature selection series

## Fixed (2025-12-12)

### Reproducibility Tracking Bug Fixes

**Critical Bug Fixes**
- **Fixed `ctx` NameError in reproducibility tracker** (`TRAINING/utils/reproducibility_tracker.py:862`)
  - **Issue**: `NameError: name 'ctx' is not defined` when saving metadata for TARGET_RANKING
  - **Impact**: Prevented `metadata.json` and `metrics.json` from being written (86 failures in `stats.json`)
  - **Fix**: Changed to use `additional_data.get('view')` instead of `getattr(ctx, 'view', None)`
  - **Result**: Metadata files now write correctly for all runs

- **Fixed feature importances save path** (`TRAINING/ranking/predictability/model_evaluation.py`)
  - **Issue**: Feature importances saved under `CROSS_SECTIONAL` even for `SYMBOL_SPECIFIC` runs
  - **Fix**: Added `view` parameter to `_save_feature_importances()` and updated directory structure
  - **Result**: Feature importances now saved under correct view directory: `target_rankings/feature_importances/{target}/{view}/{symbol?}/`

- **Fixed perfect CV detection false positive** (`TRAINING/ranking/predictability/model_evaluation.py`)
  - **Issue**: Triggered on training scores (0.9999) instead of actual CV scores (0.687)
  - **Fix**: Changed to use `primary_scores` (from `cross_val_score`) as primary source, with explicit overwrite of training metrics with CV scores
  - **Result**: Perfect CV detection now only triggers on actual validation CV scores â‰¥ 99%

- **Added missing metadata diagnostic** (`TRAINING/utils/reproducibility_tracker.py`)
  - **Enhancement**: Warns when `metadata.json` or `metrics.json` are missing but `audit_report.json` exists
  - **Purpose**: Detects partial writes from previous bugs
  - **Result**: Better visibility into metadata completeness

**Files Modified**:
- `TRAINING/utils/reproducibility_tracker.py` - Fixed `ctx` NameError, added metadata diagnostic
- `TRAINING/ranking/predictability/model_evaluation.py` - Fixed feature importances path, fixed CV score detection, updated `_compute_and_store_metrics()` to store CV scores correctly

**Impact**:
- All new runs will have complete metadata files
- Feature importances saved under correct view directories
- Perfect CV detection now accurate (no false positives)
- Existing runs from before fix are missing metadata (documented in `METADATA_MISSING_README.md`)

### Related

- See [Trend Analyzer Verification Guide](../03_technical/implementation/TREND_ANALYZER_VERIFICATION.md)
- See [Cohort-Aware Reproducibility Guide](../03_technical/implementation/COHORT_AWARE_REPRODUCIBILITY.md)
